{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.functional as F, torch.utils.data as data, torchnet as tnt\n",
    "import torchvision, numpy as np, os\n",
    "import matplotlib.pyplot as plt, fastai\n",
    "from fastai.conv_learner import ConvnetBuilder\n",
    "from fastai.model import resnet34\n",
    "import nvvl, time\n",
    "import sys, os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding layer.freeze and layer.unfreeze capability to nn.Module layers\n",
    "\n",
    "def freeze(self):\n",
    "    for param in self.parameters(): param.requires_grad=False\n",
    "def unfreeze(self):\n",
    "    for param in self.parameters(): param.requires_grad=True\n",
    "\n",
    "nn.Module.freeze=freeze\n",
    "nn.Module.unfreeze=unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"); device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/home/sufiyan/Common_data/mtp2/dataset/OLD/100_vids/\"\n",
    "PATH_L_Vids=PATH+\"LEFT/\"\n",
    "PATH_L=PATH+\"LEFT_data/\"\n",
    "PATH_L_L=PATH+\"LEFT_labels/\"\n",
    "PATH_R_Vids=PATH+\"RIGHT/\"\n",
    "PATH_R=PATH+\"RIGHT_data/\"\n",
    "PATH_R_L=PATH+\"RIGHT_labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string_to_int and int_to_string mapping\n",
    "stoi={\"w\":0, \"a\":1, \"d\":2}; itos={0:\"Clear\", 1:\"Left\", 2:\"Right\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leftVids =[PATH_L+fileName for fileName in os.listdir(PATH_L) if os.path.isfile(PATH_L+fileName)]\n",
    "rightVids=[PATH_R+fileName for fileName in os.listdir(PATH_R) if os.path.isfile(PATH_R+fileName)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myVidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, npArray, labelList):\n",
    "        assert isinstance(npArray, np.ndarray), \"npArray in myVidDataset should be a numpy array of Frames\"\n",
    "        assert len(npArray)==len(labelList), f\"Length of Labels {len(labelList)} not equal to len \\\n",
    "                                                    of npArray {len(npArray)}\"\n",
    "        self.frames, self.labels = npArray,labelList\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x=self.frames[idx]\n",
    "        x=np.squeeze(x)     #extra 1 dimention removed if any\n",
    "        if len(x.shape)==2 : x=np.dstack((x,x,x)) #id image is 1 channel, make it 3 channel\n",
    "        x=np.rollaxis(x, 2)   #Make the images channel first\n",
    "        x=x[:,:224,:224]\n",
    "        return (x, self.labels[idx])\n",
    "    \n",
    "    def __len__(self): return len(self.frames)\n",
    "    @classmethod\n",
    "    def fromPath(cls, videoPath, skipStart=100, skipEnd=100):\n",
    "        \"\"\"frames to skip at start and end to reduce class imbalance\"\"\"\n",
    "        vid=np.load(videoPath)[skipStart:-skipEnd]\n",
    "        labels=cls.get_labels(videoPath)[skipStart:-skipEnd]\n",
    "        return cls(vid, labels)\n",
    "    \n",
    "    #returns the list of labels for the passed videoPath\n",
    "    @staticmethod\n",
    "    def get_labels(fileName):\n",
    "        vidName=fileName.split(\"/\")[-1]\n",
    "        labelName=\".\".join(vidName.split(\".\")[:-1]+[\"npy\"])\n",
    "        if fileName.split(\"/\")[-2]==\"RIGHT_data\":\n",
    "            labels=list(np.load(PATH_R_L+labelName))\n",
    "        elif fileName.split(\"/\")[-2]==\"LEFT_data\":\n",
    "            labels=list(np.load(PATH_L_L+labelName))\n",
    "        else: raise ValueError(f\"Passed videoFile {fileName} has non recognizable parent folder\")\n",
    "        return [stoi[label] for label in labels]\n",
    "    \n",
    "class dataLoaderGetter(object):\n",
    "    def __init__(self, startIndex=0, endIndex=55):\n",
    "        \"\"\"get one concat dataset of left and right video each\n",
    "        Returns a new dataloader of 2 new video each from left and right each time\"\"\"\n",
    "        self.len=endIndex-startIndex\n",
    "        self.startIndex,self.endIndex = startIndex,endIndex\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i, (l,r) in enumerate(tqdm([item for item in zip(leftVids,rightVids)], total=self.len)):\n",
    "            if self.startIndex<=i and i<self.endIndex:\n",
    "                set_L,set_R=myVidDataset.fromPath(l), myVidDataset.fromPath(r)\n",
    "                dataset=torch.utils.data.ConcatDataset([set_L,set_R]) \n",
    "                yield torch.utils.data.DataLoader(dataset=dataset, batch_size=200, \n",
    "                                                  shuffle=True, pin_memory=False, num_workers=4, drop_last=True)\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads resnet34 pretrained on ImageNet using fastai library, Addaptive Pool \n",
    "#Followed by Fully connected layers of 500 and c\n",
    "model=ConvnetBuilder(resnet34, c=3, is_multi=True, is_reg=False, pretrained=True).model\n",
    "loss=nn.CrossEntropyLoss\n",
    "optim=torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the pretrained layers\n",
    "for i, layer in enumerate(model.children()):\n",
    "    if i<=11: layer.freeze()\n",
    "    else: layer.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1024, out_features=512, bias=True)\n",
      "BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Linear(in_features=512, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.children():\n",
    "    for param in layer.parameters():\n",
    "        if param.requires_grad: print(layer); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelLearner(nn.Module):\n",
    "    \"\"\"modelLearner class takes model, loss function and learning rate.\n",
    "    Given each sample (x,y), it trains on it\n",
    "    call epochEnded at the end of each epoch,\n",
    "            passing parentLearnerClassObject that has trainLoader as its attribute\n",
    "\n",
    "\"\"\"\n",
    "    def __init__(self,model, loss_fn, lr, optim, modelName, Train=True, is_multi=True, classes=3, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.loss=loss_fn().to(device)\n",
    "        self.lr=lr\n",
    "        self.model=model.to(device)\n",
    "        self.optim=optim(self.model.parameters(), self.lr)\n",
    "        self.modelName=modelName\n",
    "        self.args,self.kwargs=args,kwargs\n",
    "        self.train_epoch_loss=0     #Add loss here for each batch and reset at end of epoch\n",
    "        self.test_epoch_loss=0      #same as above for test\n",
    "        self.num_samples_seen=0\n",
    "        self.Train=Train            #Training Mode Flag\n",
    "        self.train_loss_list=[]     #to be updated at the end of  each epoch\n",
    "        self.test_loss_list=[]\n",
    "        self.is_multi=is_multi\n",
    "        if is_multi: self.confusion_matrix=tnt.meter.ConfusionMeter(classes)\n",
    "        if isinstance(self.loss, nn.MSELoss): self.loss_name=\"MSE \"\n",
    "        else: self.loss_name=\"CE \"\n",
    "        self.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        y_pred = self.model(x)\n",
    "        if self.Train==True:\n",
    "            if isinstance(self.loss, nn.CrossEntropyLoss): #Handeling specific requirements of CE Loss\n",
    "                y=y.view(self.parentLearner.trainLoader.batch_size)\n",
    "                y=y.long()\n",
    "            loss = self.loss(y_pred, y)\n",
    "            self.num_samples_seen= self.num_samples_seen + x.shape[0]\n",
    "            self.train_epoch_loss += loss.item()\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "        else: #Test Loop\n",
    "            if isinstance(self.loss, nn.CrossEntropyLoss): #Handeling specific requirements of CE Loss\n",
    "                y=y.view(self.parentLearner.validLoader.batch_size)\n",
    "                y=y.long()\n",
    "            loss = self.loss(y_pred, y)\n",
    "            self.confusion_matrix.add(y_pred.data, y.data)\n",
    "            self.test_epoch_loss+= loss.item()\n",
    "\n",
    "\n",
    "    def setTest(self):   self.Train=False\n",
    "    def setTrain(self):  self.Train=True\n",
    "    def save(self): self.model.save_state_dict(f\"saved_models/{self.modelName}_lr{self.lr}/\\\n",
    "    loss_{self.loss_name}_epoch_{len(self.train_loss_list)}.pt\")\n",
    "    \n",
    "        #setParent will give the modelLearner access the higherlevel class attribures like trainLoader's length\n",
    "    #and batch size, currentEpoch, etc\n",
    "    def setParent(self, parentLearner): self.parentLearner=parentLearner\n",
    "    def trainEpochEnded(self): \n",
    "        try:    self.train_loss_list.append(self.train_epoch_loss/\n",
    "                                            (len(self.parentLearner.trainLoader)*self.parentLearner.num_trainLoader+1))\n",
    "        except: self.train_loss_list.append(self.train_epoch_loss/len(self.parentLearner.trainLoader))\n",
    "\n",
    "        self.train_epoch_loss=0  #reset total_average_loss at the end of each epoch\n",
    "        try: \n",
    "            epochs=self.parentLearner.epochsDone\n",
    "            printEvery=self.parentLearner.printEvery\n",
    "            if epochs%printEvery==0:    \n",
    "                print(f\"lr: {self.lr}      trainLoss: {self.train_loss_list[-1]}\")\n",
    "        except: print(f\"lr: {self.lr}      trainLoss: {self.train_loss_list[-1]}\")\n",
    "    def testEpochEnded(self):\n",
    "        try:    self.test_loss_list.append(self.test_epoch_loss/\n",
    "                                           (len(self.parentLearner.validLoader)*self.parentLearner.num_validLoader+1))\n",
    "        except: self.test_loss_list.append(self.test_epoch_loss/len(self.parentLearner.validLoader))\n",
    "        self.test_epoch_loss=0\n",
    "        try: \n",
    "            epochs=self.parentLearner.epochsDone\n",
    "            printEvery=self.parentLearner.printEvery\n",
    "            if epochs%printEvery==0:\n",
    "                print(f\"lr: {self.lr}      {self.loss_name}testLoss: {self.test_loss_list[-1]}\")\n",
    "        except: print(f\"testLoss: {self.loss_name}{self.test_loss_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelLearner(nn.Module):\n",
    "    \"\"\"ParallelLearner takes list of modelLearners to be trained parallel on the same data samples\n",
    "    from the passed trainLoader object. epochs are the number of epochs to be trained for.\n",
    "    \"\"\"\n",
    "    def __init__(self, listOfLearners, epochs, trainLoaderGetter=None, trainLoader=None, printEvery=10, validLoader=None, validLoaderGetter=None, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.learners=listOfLearners\n",
    "        self.trainLoader=trainLoader\n",
    "        self.trainLoaderGetter=trainLoaderGetter\n",
    "        self.epochs=epochs\n",
    "        self.args,self.kwargs = args,kwargs\n",
    "        self.validLoader=validLoader           #trainLoader for test set\n",
    "        self.validLoaderGetter=validLoaderGetter\n",
    "        self.epochsDone=0  #epoch counter\n",
    "        self.printEvery=printEvery #print every n epochs\n",
    "        try: [learner.setParent(self) for learner in self.learners] #set self as parent of all modelLearners\n",
    "        except: print(\"Couldn't set ParallelLearner as parent of modelLearners!!!\")\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        startTime=time.time()\n",
    "        for t in range(self.epochs):\n",
    "            [learner.setTrain() for learner in self.learners] #set all modelLearners to Train Mode\n",
    "            for self.num_trainLoader, self.trainLoader in enumerate(self.trainLoaderGetter()):\n",
    "                for idx, (x,y) in enumerate(self.trainLoader):\n",
    "#                     x = x.view(self.trainLoader.batch_size,28*28).to(device)\n",
    "#                     y = y.view(self.trainLoader.batch_size, 1).float().to(device)\n",
    "                    x = x.float().to(device)\n",
    "                    y = y.float().to(device)\n",
    "                    [learner(x,y) for learner in self.learners]\n",
    "            self.epochsDone+=1\n",
    "            if self.epochsDone%self.printEvery==0:\n",
    "                print()\n",
    "                print(\"*\"*50)\n",
    "                print(f\"Epoch: {t}   Time Elapsed: {time.time()-startTime}\")\n",
    "            [learner.trainEpochEnded() for learner in self.learners]\n",
    "            if (not self.validLoaderGetter is None): #This part runs only when validLoaderGetter is provided\n",
    "                [learner.setTest() for learner in self.learners] #Set all modelLearners to Test Model\n",
    "                for self.num_validLoader, self.validLoader in enumerate(self.validLoaderGetter()):\n",
    "                    for idx, (x,y) in enumerate(self.validLoader):\n",
    "                        x = x.float().to(device)\n",
    "                        y = y.float().to(device)\n",
    "                        [learner(x,y) for learner in self.learners]\n",
    "                [learner.testEpochEnded() for learner in self.learners]\n",
    "        #Pass self to all learners defined above so they can use self.trainLoader to calculate it's total_loss before resetting epoch_loss\n",
    "    \n",
    "    \n",
    "    def plotLoss(self, title, listOfLabelsForTrain, listOfLabelsForTest=None, xlabel=\"Epochs\", ylabel=\"Loss\", save=False):\n",
    "        \"\"\"Parameters:\n",
    "        listOfLabelsForTrain: Labels for the train epoch loss for each modelLearner\n",
    "        listOfLabelsForTest : Labels for the test epoch loss for each modelLearner, \\\n",
    "                              to be provided if validLoader was used to calculate loss on validation dataset.\n",
    "        \"\"\"\n",
    "        assert len(listOfLabelsForTrain)==len(self.learners), \"Provide Description for all Learners to Plot\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import os\n",
    "        plt.switch_backend('agg')\n",
    "        x=range(1,self.epochsDone+1)\n",
    "        for i,learner in enumerate(self.learners):\n",
    "            plt.plot(x, learner.train_loss_list, label=listOfLabelsForTrain[i])\n",
    "        if (not (listOfLabelsForTest is None)) and (not (self.validLoader is None)):\n",
    "            assert len(listOfLabelsForTest)==len(self.learners), \\\n",
    "                        \"length of ListOfLabelsForTest is not same as num of learners\"\n",
    "            for i,learner in enumerate(self.learners):\n",
    "                plt.plot(x, learner.test_loss_list, label=listOfLabelsForTest[i])\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        os.makedirs(\"plots\", exist_ok=True)\n",
    "        plt.savefig(os.path.join(\"plots\", title+\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "mainLearner=ParallelLearner([modelLearner(model, lr=0.01,loss_fn=nn.CrossEntropyLoss, optim=optim, modelName=\"onVivekData\")],\n",
    "                            epochs=20,\n",
    "                            trainLoaderGetter=dataLoaderGetter,\n",
    "                            printEvery=10,\n",
    "                            validLoaderGetter=partial(dataLoaderGetter, startIndex=56, endIndex=63))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:44,  1.38it/s]                        \n",
      "62it [00:04, 12.86it/s]              \n",
      "62it [00:44,  1.38it/s]                        \n",
      "62it [00:04, 12.78it/s]              \n",
      "62it [00:44,  1.39it/s]                        \n",
      "62it [00:04, 12.78it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.76it/s]              \n",
      "62it [00:44,  1.39it/s]                        \n",
      "62it [00:04, 12.81it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.49it/s]              \n",
      "62it [00:45,  1.38it/s]                        \n",
      "62it [00:04, 12.73it/s]              \n",
      "62it [00:45,  1.38it/s]                        \n",
      "62it [00:04, 12.93it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.88it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Epoch: 9   Time Elapsed: 494.1451003551483\n",
      "lr: 0.01      trainLoss: 0.6508806014279707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:04, 12.70it/s]              \n",
      "  0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.01      CE testLoss: 0.7518012577837164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:45,  1.38it/s]                        \n",
      "62it [00:04, 12.70it/s]              \n",
      "62it [00:44,  1.38it/s]                        \n",
      "62it [00:04, 12.59it/s]              \n",
      "62it [00:44,  1.38it/s]                        \n",
      "62it [00:04, 12.79it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.85it/s]              \n",
      "62it [00:44,  1.38it/s]                        \n",
      "62it [00:04, 12.86it/s]              \n",
      "62it [00:45,  1.38it/s]                        \n",
      "62it [00:04, 12.88it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.87it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.83it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "62it [00:04, 12.73it/s]              \n",
      "62it [00:45,  1.37it/s]                        \n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Epoch: 19   Time Elapsed: 993.5646302700043\n",
      "lr: 0.01      trainLoss: 0.6224833546428505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:04, 12.81it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.01      CE testLoss: 0.7630406672304327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mainLearner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=mainLearner.learners[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2073,    0,    0],\n",
       "       [ 169,    0,   13],\n",
       "       [ 144,    0,    1]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.confusion_matrix.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainLearner.plotLoss(\"myLossFor5Epoch\", [\"train\"], [\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mp): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (9): Flatten()\n",
       "  (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Dropout(p=0.25)\n",
       "  (12): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (13): ReLU()\n",
       "  (14): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): Dropout(p=0.5)\n",
       "  (16): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (17): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGetter = dataLoaderGetter(56,57)\n",
    "dataLoader=next(iter(testGetter))\n",
    "print(dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=next(iter(dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=xy\n",
    "x = x.float().to(device)\n",
    "y = y.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=learner.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
